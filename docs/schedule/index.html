<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Talk Schedule</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="useR, hugo, conference">	
  

  
  <meta name="description" content="The conference for users of R from July 10-13, 2018 held in Brisbane, Australia.">	
  

  <meta name="generator" content="Hugo 0.82.0" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="https://user2018.r-project.org/css/animate.css" rel="stylesheet">

  
  
    <link href="https://user2018.r-project.org/css/style.blue.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="https://user2018.r-project.org/css/custom.css" rel="stylesheet">

  
    

  
  <link rel="shortcut icon" href="https://user2018.r-project.org/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="https://user2018.r-project.org/img/apple-touch-icon.png" />
  

  <link href="https://user2018.r-project.org/css/owl.carousel.min.css" rel="stylesheet">
  <link href="https://user2018.r-project.org/css/owl.theme.default.min.css" rel="stylesheet">

  <link rel="alternate" href="https://user2018.r-project.org/index.xml" type="application/rss+xml" title="useR! 2018">

  
  <meta property="og:title" content="Talk Schedule" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/schedule//" />
  <meta property="og:image" content="img/user_au_logo.png" />

</head>


  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="https://user2018.r-project.org/">
                    <img src="https://user2018.r-project.org/img/user_au_logo_small.png" alt="Talk Schedule logo" class="hidden-xs hidden-sm">
                    <img src="https://user2018.r-project.org/img/user_au_logo_small.png" alt="Talk Schedule logo" class="visible-xs visible-sm">
                    <span class="sr-only">Talk Schedule - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Programme <span class="caret"></span></a>
                    <ul class="dropdown-menu">
                      
                        <li><a href="/scientific/">Scientific Programme</a></li>
                      
                        <li><a href="/schedule/">Talk Schedule</a></li>
                      
                        <li><a href="/lightning/">Lightning Talk Schedule</a></li>
                      
                        <li><a href="/poster/">Poster Schedule</a></li>
                      
                        <li><a href="/presenter/">Info for presenters</a></li>
                      
                        <li><a href="/tutorials/">Tutorials</a></li>
                      
                        <li><a href="/rcurious/">FREE Tutorial</a></li>
                      
                        <li><a href="/datathon/">Datathon</a></li>
                      
                        <li><a href="/social/">Social Programme</a></li>
                      
                    </ul>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Registration <span class="caret"></span></a>
                    <ul class="dropdown-menu">
                      
                        <li><a href="/registration/">Register</a></li>
                      
                        <li><a href="/abstract/">Abstract Submission</a></li>
                      
                        <li><a href="/scholarship/">Scholarship</a></li>
                      
                    </ul>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/code_of_conduct/">Code of Conduct</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">News</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                      <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Travel <span class="caret"></span></a>
                    <ul class="dropdown-menu">
                      
                        <li><a href="/travel/">Travel to/from/around</a></li>
                      
                        <li><a href="/hotels/">Hotels</a></li>
                      
                        <li><a href="/visit/">Things to do</a></li>
                      
                    </ul>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/faq/">FAQ</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/contact/">Contact</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Talk Schedule</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            

            <div class="container">

                <div class="row">

                    <div class="col-md-9">

                        <div>
                          <p><strong>The talks will take place on 11-13 July 2018 (click the interested talk for its abstract). A datatable version is provided <a href="../schedule-dt.html">here</a>, if you&rsquo;re looking for a more easy-to-search &amp; R-oriented format.</strong> Information for presenters is <a href="../presenter.html">here</a>.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>  &lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;1&quot; data-target=&quot;.1collapsed&quot;&gt;
</code></pre>
<!-- raw HTML omitted -->
<pre><code>&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;48&quot; data-target=&quot;.48collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Applications in society&lt;/td&gt;  &lt;td&gt;Richard Layton&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;Data, methods, and metrics for studying student persistence&lt;/td&gt;  &lt;td&gt;applications, community/education, persistence metrics, intersectionality, longitudinal&lt;/td&gt;  &lt;td&gt;Jessie Roberts&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://speakerdeck.com/graphdr/midfieldr-data-methods-and-metrics-for-studying-student-persistence&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 48collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;This paper introduces R users to data and tools for investigating undergraduate persistence metrics using the midfieldr package and its associated data packages. The data are the student records (registrar's data) of approximately 200,000 undergraduates at US institutions from 1990 to 2016. midfieldr provides functions for determining persistence metrics such as graduation rates and for grouping and displaying findings by program, institution, race/ethnicity, and sex. These packages provide an entry to this type of intersectional research for anyone with basic proficiency in R and familiarity with packages from the tidyverse. The goal of the paper is to introduce the packages and to share our data, methods, and metrics for intersectional research in student persistence.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;28&quot; data-target=&quot;.28collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Applications in society&lt;/td&gt;  &lt;td&gt;Maria Holcekova&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;The dynamic approach to inequality: Using longitudinal trajectories of young women and their parents in determining their socio-economic positions within the contemporary Western society&lt;/td&gt;  &lt;td&gt;visualisation, clustering, imputation, longitudinal data analysis&lt;/td&gt;  &lt;td&gt;Jessie Roberts&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 28collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Intensified globalisation and ensuing increased affluence of Western populations has changed the composition of traditional social class system in England. This does not imply the disappearance of socio-economic (SE) classes and inequalities, but rather their redefinition. Unfortunately, limited research has considered the dynamic nature of SE positions, especially in understanding youth transitions from parental to personal SE classes. I address this problem using nationally representative longitudinal data in the Next Steps 1990 youth cohort study in England. Firstly, I explore the parental transition patterns using longCatPlot. Secondly, I visualise missing data through the missmap function in Amelia and impute these values using random forests in missForest. Thirdly, I employ the daisy function within the cluster to create SE groups based on Gower distance, partitioning around medoids, and silhouette width. Finally, I visualise these results using ggplot2. In doing so, I establish five distinct SE groups of young women that contributes to the understanding of new forms of inequality, and I discuss its implications in terms of access to educational and labour market resources.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;15&quot; data-target=&quot;.15collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Applications in society&lt;/td&gt;  &lt;td&gt;Frank C.S. Liu&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;The Second Wing of Polls: How Multiple Correspondence Analysis using R Advances Exploring Associated Attitudes in Smaller-Data&lt;/td&gt;  &lt;td&gt;applications&lt;/td&gt;  &lt;td&gt;Jessie Roberts&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 15collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Polls and surveys have been used for better forecasting voter preferences and understanding consumer behavior. Academically we employ the strength of these smaller but representative data to confirm theory, including identifying associations between theoretically identified variables. However, researchers who like to explore new patterns for better understanding voters' behavior and attitudes are hardly satisfied by the current practice of survey data analysis. While we turn to bigger data, little attention has been preserved to the value of such smaller data for their potential to achieve the same goal. This talk will demonstrate how the use of &quot;FactorMineR&quot; package of R assists exploration of associated concepts and attitudes and patterns that could not be identified by theories in the first place. Implications for the practice of survey data collection and MCA's connection to association rules mining will be discussed.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;71&quot; data-target=&quot;.71collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Applications in society&lt;/td&gt;  &lt;td&gt;Meryam Krit&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;Modelling Rift Valley Fever&lt;/td&gt;  &lt;td&gt;models, applications, community/education&lt;/td&gt;  &lt;td&gt;Jessie Roberts&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://www.dropbox.com/s/4q5p4a2b6xhule1/useR2018-MK.pdf?dl=0&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 71collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Rift Valley Fever (RVF) is one of the major viral zoonoses in Africa, affecting man and several domestic animal species. The epidemics generally involve a 5 – 15 year cycle marked by abnormally high rainfall (El Niño/Southern Oscillation phenomenon (ENSO)), but there is more and more evidence of inter-epidemic transmission.A flexible model describing RVF transmission dynamics in six species (human, domestic animal, four vectors) in three different areas will be presented. The model allows for migration, flooding, variation in climate, seasonal effects on vector egg hatching, transhumance, alternative wildlife hosts and increased susceptibility of animals.User-friendly shiny interface and optimized Rcpp implementation allow the epidemiological researchers to study different scenarios and adapt it to other situations. Application of the model  to the specific situation in Tanzania and Algeria will be discussed.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;52&quot; data-target=&quot;.52collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Big data&lt;/td&gt;  &lt;td&gt;Miguel Gonzalez-Fierro&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Spark on demand with AZTK&lt;/td&gt;  &lt;td&gt;big data&lt;/td&gt;  &lt;td&gt;Max Kuhn&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 52collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Apache Spark has become the technology of choice for big data engineering. However, provisioning and maintaining Spark clusters can be challenging and expensive. To address this issue, Microsoft has developed the Azure Distributed Data Engineering Toolkit (AZTK). This talk describes how AZTK Spark clusters can be provisioned in the cloud from a local machine with just a few commands. The clusters are ready to use in under 5 minutes and come with R and R Studio Server pre-installed, allowing R users to start developing Spark applications immediately. Users can apply their own Docker image to customize the Spark environment. ATZK clusters, composed of low priority Azure virtual machines, can be created on demand and run only as needed allowing for large cost savings. We will show a short demo of how the pre-installed sparklyr package can be used to perform data engineering tasks using dplyr syntax, and machine learning using the Spark MLlib library.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;76&quot; data-target=&quot;.76collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Big data&lt;/td&gt;  &lt;td&gt;Benjamin Ortiz Ulloa&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Graphs: Datastructures to Query&lt;/td&gt;  &lt;td&gt;algorithms, models, databases, networks, text analysis/NLP, big data&lt;/td&gt;  &lt;td&gt;Max Kuhn&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 76collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;When people think of graphs, they often think about mapping out social media connections. While graphs are indeed useful for mapping out social networks, they have many other practical applications. Data in the real world resemble vertices and edges more than they resemble rows and columns.  This allows researchers to intuitively grasp the data modeled and stored within a graph. Graph exploration -- also known as graph traversal -- is traditionally done with a traversal language such as Gremlin or Cypher. The functionality of these traversal languages can be duplicated by combining the  igraph and magrittr packages. Traversing a graph in R gives useRs access to a myriad of simple, but powerful algorithms to explore their data sets. This talk will show why data should be explored as a graph as well as show how a graph can be traversed in R. I will do this by going through a survey of different graph traversal techniques and by showing the code patterns necessary for each of those techniques.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;147&quot; data-target=&quot;.147collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Big data&lt;/td&gt;  &lt;td&gt;Amy Stringer Amy Stringer&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Automated Visualisations for Big Data&lt;/td&gt;  &lt;td&gt;visualisation, reproducibility, big data&lt;/td&gt;  &lt;td&gt;Max Kuhn&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 147collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The Catlin Seaview project is a large scale reef survey for estimating coralcover at various locations around the world. Upon re-surveying, it is possibleto track changes in, and predict the future condition of, these reefs over time.The survey collects hundreds of thousands of images from 2km transects ofreef, which are then sent to a neural network for automatic annotation of reefcommunities. Annotations are completed in such a way that the resulting datahave hierarchical spatial scales; going up from image, to transect, to reef, tosubregion, to region.Here, we present an efficient method for extracting, summarising and visu-alising the big and complex data with Rmarkdown, dplyr and ggplot2. The useof Rmarkdown, for report generation, allows for the introduction of parametersinto the construction of the document, allowing for entirely unique reports to bedeveloped from the one source script. This approach has resulted in a systemfor compiling 22 reproducible reports, extracting, summarising and visualisingdata at multiple spatial scales, from over 600 000 images, in a matter of minutes;leaving machines to do the work so that people have time to think.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;199&quot; data-target=&quot;.199collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Big data&lt;/td&gt;  &lt;td&gt;Snehalata Huzurbazar&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Visualizations to guide dimension reduction for sparse high-dimensional data&lt;/td&gt;  &lt;td&gt;visualisation&lt;/td&gt;  &lt;td&gt;Max Kuhn&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 199collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Dimension reduction for high-dimensional data is necessary for descriptive data analysis. Most researchers restrict themselves to visualizing 2 or 3 dimensions, however, to understand relationships between many variables in high-dimensional data, more dimensions are needed. This talk presents several new options for visualizing beyond 3D. These are illustrated using 16S rRNA microbiome data. We will show intensity plots developed to highlight the changing contributions of taxa (or subjects) as the number of principal components of the dimension reduction or ordination method  are changed. And secondarily revive Andrews curves, connected with a tour algorithm for viewing 1D projections of multiple principal components, to study group behavior in the high-dimensional data. The plots provide a quick visualization of taxa/subjects that are close to the `center' or that contribute to dissimilarity. They also allow for exploration of patterns among related subjects or taxa not seen in other visualizations. All code is written in R and available on Github.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;20&quot; data-target=&quot;.20collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Stat methods for high-dim biology&lt;/td&gt;  &lt;td&gt;Florian Rohart&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;mixOmics: An R package for 'omics feature selection and multiple data integration&lt;/td&gt;  &lt;td&gt;data mining, applications, bioinformatics, multivariate, big data&lt;/td&gt;  &lt;td&gt;Julie Josse&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 20collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The mixOmics R-package contains a suite of multivariate methods that model molecular features holistically and statistically integrate diverse types of data (e.g. ‘omics data as transcriptomics, proteomics, metabolomics) to offer an insightful picture of a biological system.Our two latest frameworks for data integration; N-integration with DIABLO combines different ‘omics datasets measured on the same N samples or individuals; P-integration with MINT combines studies measured on the same P features (e.g., genes) but from independent cohorts of individuals. Both frameworks are introduced in a discriminative context for the identification of relevant and robust molecular signatures across multiple data sets. mixOmics is a well-designed, user-friendly package with attractive graphical outputs. It represents a significant contribution to the field of computational biology which has a strong need for such toolkits to mine and integrate datasets. &lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;61&quot; data-target=&quot;.61collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Stat methods for high-dim biology&lt;/td&gt;  &lt;td&gt;Claus Ekstrøm&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;Using mommix for fast, large-scale genome-studies in the presence of gene-environment and gene-gene interaction&lt;/td&gt;  &lt;td&gt;algorithms, models, bioinformatics, big data&lt;/td&gt;  &lt;td&gt;Julie Josse&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 61collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The majority of disorders and outcomes analysed in genome-wide association studies are believed to multi-factorial and influenced by gene-environment (GxE), gene-gene (GxG) interactions, or both. However, including GxE or GxG increases the computational burden by several orders of magnitude which makes the inclusion of interactions prohibitively cumbersome.Finite mixtures of regression models provide a flexible modeling framework for many phenomena. Using moment-based estimation of the regression parameters, we develop unbiased estimators with a minimum of assumptions on the mixture components. In particular, only the average regression model for one of the components in the mixture model is needed and no requirements on any of the distributions.We present a new R package, mommix, for moment-based mixtures of regression models, which implements this new approach for regression mixtures. We illustrate the use of the moment-based mixture of regression models with an application to genome-wide association analysis, and show that the implementation is fast, which makes large-scale genetic analysis with gene-environment and gene-gene interactions feasible.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;32&quot; data-target=&quot;.32collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Stat methods for high-dim biology&lt;/td&gt;  &lt;td&gt;Jacob Bergstedt&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;﻿Quantifying the immune system with the MMI package&lt;/td&gt;  &lt;td&gt;models, data mining, applications, reproducibility, bioinformatics, interfaces&lt;/td&gt;  &lt;td&gt;Julie Josse&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 32collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The blood composition of immune cells provide a key indicator of human health and disease. To identify the sources of variation in this composition, we combined standardized flow cytometry and a questionnaire investigating demographical factors in 816 French individuals. The study is published in the Nature immunology article “Natural variation in innate immune cell parameters is preferentially driven by genetic factors”.To facilitate the study, we developed the R package MMI (https://github.com/jacobbergstedt/mmi), which defines a framework to specify a family of models. Operations are implemented for models in the family, such as doing tests, computing confidence intervals or AIC measures and investigating residuals, the results of which are collected in a MapReduce-like pattern. The software keeps track of variables, parameter transformations, multiple testing and selective inference adjustments.With the package we release the dataset of 816 observations of 166 immune cell parameters and 44 demographical variables. We hope that this resource can be used to generate hypotheses in immunology, but also be of benefit to the broader community, in education and benchmarking.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;122&quot; data-target=&quot;.122collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Stat methods for high-dim biology&lt;/td&gt;  &lt;td&gt;Rudradev Sengupta&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;High Performance Computing Using R for High Dimensional Surrogacy Applications in Drug Development&lt;/td&gt;  &lt;td&gt;models, data mining, applications, bioinformatics, performance, big data&lt;/td&gt;  &lt;td&gt;Julie Josse&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 122collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Identification of genetic biomarkers is a primary data analysis task in the context of drug discovery experiments. These experiments consist of several high dimensional datasets which contain information about a set of new drugs under development. This type of data structure introduces the challenge of multi-source data integration which is needed in order to identify the biological pathways related to the new set of drugs under development. In order to process all the information contained in the datasets, high performance computing techniques are required. Currently available R packages, for parallel computing, are not optimized for a specific setting and data structure. We proposed a new “master-slave” framework, for data analysis using R in parallel, in a computer cluster. The proposed data analysis workflow is applied to a multi-source high dimensional drug discovery dataset and a performance comparison is made between the new framework and existing R packages for parallel computing. Different configuration settings,  for parallel programming in R, are presented  to show that the computation time, for the specific application under consideration, can be reduced by 534.62%.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;34&quot; data-target=&quot;.34collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Robust methods&lt;/td&gt;  &lt;td&gt;Kasey Jones&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;rollmatch: An R Package for Rolling Entry Matching&lt;/td&gt;  &lt;td&gt;algorithms, models&lt;/td&gt;  &lt;td&gt;Adam Sparks&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 34collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The gold standard of experimental research is the randomized-control trial. However, many healthcare interventions are implemented without a randomized control group for practical or ethical reasons. Propensity score matching (PSM) is a popular method for approximating a randomized experiment from observational data by matching members of a treatment group to similar candidates of a control group that did not receive the intervention. However, traditional PSM is not designed for studies that enroll participants on a rolling basis, a common practice in healthcare interventions where delaying treatment may impact patient health. Rolling Entry Matching (REM) is a new matching method that addresses the rolling entry problem by selecting comparison group members who are similar to intervention members with respect to both static, unchanging characteristics (e.g., race, DOB) and dynamic characteristics that change over time (e.g., health conditions, health care use). This presentation will introduce both REM and rollmatch, an R package for performing REM to assess rolling entry interventions.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;57&quot; data-target=&quot;.57collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Robust methods&lt;/td&gt;  &lt;td&gt;Charles T. Gray&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;varameta': Meta-analysis of medians&lt;/td&gt;  &lt;td&gt;algorithms, models, applications, reproducibility&lt;/td&gt;  &lt;td&gt;Adam Sparks&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 57collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Meta-analyses bring together summary statistics from multiple sources; which are reported in various ways. In this talk I will introduce the `varameta` package, which will provide an underlying (and reproducible) framework for understanding skewed meta-analysis data and reporting. The `varameta` package accompanies a couple of theoretical meta-analysis papers I am working on for meta-analysis of medians. This package is also designed to be an adjunct to the well-established conventional `metafor` package. In this package I have collated the existing techniques for meta-analysing skewed data reported as medians and interquartile ranges (or ranges). The `varameta` package will also include reproducible simulation documentation (in .Rmd) of existing methods in meta-analysis benchmarked against our proposed estimator for the standard error of the sample median. In this talk I will demonstrate the package, the web interface for clinicians, as well as how it can be implemented in everyday systematic reviews.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;120&quot; data-target=&quot;.120collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Robust methods&lt;/td&gt;  &lt;td&gt;Sevvandi Kandanaarachchi&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;Does normalizing your data affect outlier detection?&lt;/td&gt;  &lt;td&gt;algorithms, Data pre-processing&lt;/td&gt;  &lt;td&gt;Adam Sparks&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://github.com/sevvandi/Outliers-Norm-Instance/blob/master/Sevvandi_UseR2018.pdf&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 120collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;It is common practice to normalize data before using an outlier detection method. But which method should we use to normalize the data? Does it matter? The short answer is yes, it does. The choice of normalization method may increase or decrease the effectiveness of an outlier detection method on a given dataset. In this talk we investigate this triangular relationship between datasets, normalization methods and outlier detection methods.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;180&quot; data-target=&quot;.180collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Robust methods&lt;/td&gt;  &lt;td&gt;Priyanga Dilini Talagala&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;oddstream and stray: Anomaly Detection in Streaming Temporal Data with R&lt;/td&gt;  &lt;td&gt;algorithms, space/time, multivariate, streaming data, outlier detection&lt;/td&gt;  &lt;td&gt;Adam Sparks&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 180collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;This work introduces two R packages, oddstream and stray for detecting anomalous series within a large collection of time series in the context of non-stationary streaming data.     In `oddstream` we define an anomaly as an observation that is very unlikely given the recent distribution of a given system. This package provides a framework that provides early detection of anomalous behaviour within a large collection of streaming time series.  This includes a novel approach that adapts to non-stationarity.     In `stray` we define an anomaly as an observation that deviates markedly from the majority with a large distance gap.  This package provides a framework to detect anomalies in high dimensional data. Then the  framework is extended to identify anomalies in streaming temporal data.    The proposed algorithms use time series features as inputs, and approaches based on extreme value theory for the model building process. Using various synthetic and real datasets, we demonstrate the wide applicability and usefulness of our proposed frameworks. We show that the proposed algorithms can work well in the presence of noisy non-stationary data within multiple classes of time series.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;94&quot; data-target=&quot;.94collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Reproducibility&lt;/td&gt;  &lt;td&gt;John Blischak&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;The workflowr R package: a framework for reproducible and collaborative data science&lt;/td&gt;  &lt;td&gt;reproducibility&lt;/td&gt;  &lt;td&gt;Scott Came&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://speakerdeck.com/jdblischak/the-workflowr-r-package-a-framework-for-reproducible-and-collaborative-data-science&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 94collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The workflowr R package helps scientists organize their research in a way that promotes effective project management, reproducibility, collaboration, and sharing of results. workflowr combines literate programming (knitr and rmarkdown) and version control (Git, via git2r) to generate a website containing time-stamped, versioned, and documented results. Any R user can quickly and easily adopt workflowr, which includes four key features: (1) workflowr automatically creates a directory structure for organizing data, code, and results; (2) workflowr uses the version control system Git to track different versions of the code and results without the user needing to understand Git syntax; (3) to support reproducibility, workflowr automatically includes code version information in webpages displaying results and; (4) workflowr facilitates online Web hosting (e.g. GitHub Pages) to share results. Our goal is that workflowr will make it easier for scientists to organize and communicate reproducible research results. Documentation and source code are available at https://github.com/jdblischak/workflowr.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;157&quot; data-target=&quot;.157collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Reproducibility&lt;/td&gt;  &lt;td&gt;Peter Baker&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Efficient data analysis and reporting: DRY workflows in R&lt;/td&gt;  &lt;td&gt;applications, reproducibility&lt;/td&gt;  &lt;td&gt;Scott Came&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 157collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;When analysing data for different projects, do you often find yourself repeating the same steps? Typically, these steps follow a familiar pattern of reading, cleaning, summarising, plotting and analysing data then producing a report. To aid reproducibility, naive examples using Rmarkdown are often presented. However, I  routinely employ a modular approach combining GNU Make, R, Rmarkdown and/or Sweave files tracked under git. This system helps to implement a don't repeat yourself (DRY) approach and scales up well as projects become more complex.To aid automation, I have developed generic R, Rmarkdown, STATA, SAS and other pattern rules for GNU Make as well as R packages to generate a project skeleton consisting of initial directories, Makefiles, R syntax files for basic data cleaning and summaries; move data files and documents to standard directories; use codebook information to specify factors and check data; and finally initialise and add these to a local git repository. Comparisons will be made with alternate approaches such as ProjectTemplate and drake.GNU Make pattern rules and R software are available at https://github.com/petebaker.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;98&quot; data-target=&quot;.98collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Reproducibility&lt;/td&gt;  &lt;td&gt;Filip Krikava&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Automated unit test generation using genthat&lt;/td&gt;  &lt;td&gt;reproducibility, testing&lt;/td&gt;  &lt;td&gt;Scott Came&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;http://bit.ly/user18-genthat&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 98collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Your package has examples and vignettes of its overall functionality but no unit tests for individual functions. Writing those is no fun. Yet, when something goes wrong, unit tests are your best tool to quickly pinpoint errors. The genthat package can generate unit tests for you in the popular testthat format. Moreover, it can also be used to create reproductions when you find a bug in someone else’s code. There, instead of generating passing test cases it will generate the smallest, purposefully failing, one.Genthat does not magically create new tests out of the blue, instead it simply extracts the smallest possible test fragments from existing code. It does that by recording the input arguments and return values of all function called by clients of your package. The generated tests concentrate on single functions and test them independently of each other. Therefore a failing test usually locates the error more precisely that a failing chunk of application code. Trying it out on random set of 1500 CRAN packages, genthat managed to reproduce 80% of all function calls, increasing the unit test coverage from 19% to 54%. In this talk we present genthat and discuss testing R code.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;145&quot; data-target=&quot;.145collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Reproducibility&lt;/td&gt;  &lt;td&gt;Dan Wilson&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Practical R Workflows&lt;/td&gt;  &lt;td&gt;reproducibility, workflow&lt;/td&gt;  &lt;td&gt;Scott Came&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://www.slideshare.net/DanWilson139/practical-workflows-in-r&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 145collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Learn how R can be used to create reproducible workflows for practical use in business. As analysts and data scientists we often need to repeat our work time and time again. Sometimes this will be the exact same task, other times it may be a slight variation for another client or stakeholder. This talk will demonstrate a real-world set of workflows established at The Data Collective designed to reduce the amount of copy/paste type actions to a few function calls that get the repetitive actions out of the way, so you can focus on the important parts of your job. Find out how to overcome the challenges of a repeatable workflow and make your life easier.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;23&quot; data-target=&quot;.23collapsed&quot;&gt;  &lt;td&gt;15:30&lt;/td&gt;  &lt;td&gt;Spatial data and modeling&lt;/td&gt;  &lt;td&gt;Matt Moores&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;bayesImageS: an R package for Bayesian image analysis&lt;/td&gt;  &lt;td&gt;algorithms, applications, space/time&lt;/td&gt;  &lt;td&gt;Dale Bryan-Brown&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 23collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;There are many approaches to Bayesian computation with intractable likelihoods, including the exchange algorithm, approximate Bayesian computation (ABC), thermodynamic integration, and composite likelihood. These approaches vary in accuracy as well as scalability for datasets of significant size. The Potts model is an example where such methods are required, due to its intractable normalising constant. This model is a type of Markov random field, which is commonly used for image segmentation. The dimension of its parameter space increases linearly with the number of pixels in the image, making this a challenging application for scalable Bayesian computation. My talk will introduce various algorithms in the context of the Potts model and describe their implementation in C++, using OpenMP for parallelism. I will also discuss the process of releasing this software as an open source R package on the CRAN repository.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;90&quot; data-target=&quot;.90collapsed&quot;&gt;  &lt;td&gt;15:50&lt;/td&gt;  &lt;td&gt;Spatial data and modeling&lt;/td&gt;  &lt;td&gt;Jin Li&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;A new R package for spatial predictive modelling: spm&lt;/td&gt;  &lt;td&gt;models, data mining, reproducibility, space/time, performance, spatial predictive models; hybrid methods of geostatistics and machine learning; model selection and validation; predictive accuracy&lt;/td&gt;  &lt;td&gt;Dale Bryan-Brown&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Jin_Li32&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 90collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Accuracy of spatial predictions is crucial for evidence-informed environmental management and conservation. Improving the accuracy by identifying the most accurate predictive model is essential, but also challenging as the accuracy is affected by multiple factors. Recently developed hybrid methods of machine learning methods and geostatistics have shown their advantages in spatial predictive modelling in environmental sciences, with significantly improved predictive accuracy. An R package, ‘spm: Spatial Predictive Modelling’, has been developed to introduce these methods and recently released for R users. This presentation will briefly introduce spm, including: 1) spatial predictive methods, 2) new hybrid methods of geostatistical and machine learning methods, 3) assessment of predictive accuracy, 4) applications of spatial predictive models, and 5) relevant functions in spm. It will then demonstrate how to apply some functions in spm to relevant datasets and to show the resultant improvements in predictive accuracy and modelling efficiency. Although in this presentation, spm is applied to data in environmental sciences, it can also be applied to data in other relevant disciplines.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;127&quot; data-target=&quot;.127collapsed&quot;&gt;  &lt;td&gt;16:10&lt;/td&gt;  &lt;td&gt;Spatial data and modeling&lt;/td&gt;  &lt;td&gt;Daniel Fryer&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;rcosmo: Statistical Analysis of the Cosmic Microwave Background&lt;/td&gt;  &lt;td&gt;visualisation, databases, space/time, big data, new R package&lt;/td&gt;  &lt;td&gt;Dale Bryan-Brown&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 127collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The Cosmic Microwave Background (CMB) is remnant electromagnetic radiation from the epoch of recombination. It is the most ancient important source of data about the early universe and the key to unlocking the mysteries of the Big Bang and the structure of time and space. Spurred on by a wealth of satellite data, intensive investigations in the past few years have resulted in many physical and mathematical results to characterise CMB radiation. It can be modelled as a realisation of a homogeneous Gaussian random field on the sphere. But, what does any of this matter for statisticians if they cannot play with the CMB data in their favourite programming language?A new R package, rcosmo, provides easy access to the CMB data and various tools for exploring geometric and statistical properties of the CMB. This talk will be a quick introduction to rcosmo by one of its developers, followed by an invitation for discussions and suggestions.This research was supported under the Australian Research Council's Discovery Project DP160101366.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;210&quot; data-target=&quot;.210collapsed&quot;&gt;  &lt;td&gt;16:30&lt;/td&gt;  &lt;td&gt;Spatial data and modeling&lt;/td&gt;  &lt;td&gt;Marek Rogala&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;Using deep learning on Satellite imagery to get a business edge&lt;/td&gt;  &lt;td&gt;visualisation, algorithms, models, applications, web app, Satellite data&lt;/td&gt;  &lt;td&gt;Dale Bryan-Brown&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 210collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The talk is about new possibilities arising from applying deep learning to satellite imagery. Satellite data changes the game as it allows to reach information not available to business nowadays and to travel in time. Combined with deep learning techniques, it delivers unique insights that have never been available before.Using deep learning on satellite data can deliver insights no human can. Satellite data is huge and non-obvious. By being able to go back to an arbitrary time in history we can prevent frauds. We can build forecasts and observe events we wouldn’t have access to otherwise. We’ll explore a number of emerging use cases and the common traits behind them. I will show how our R department is working with satellite data and how we use Shiny to build decision support systems for business.As an example of my previous talks, here’s a link of my talk at UseR, Brussel 2017: https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/shinycollections-Google-Docs-like-live-collaboration-in-Shiny&lt;/td&gt;&lt;/tr&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>  &lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;2&quot; data-target=&quot;.2collapsed&quot;&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;84&quot; data-target=&quot;.84collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Applications in health and environment&lt;/td&gt;  &lt;td&gt;Mark Padgham&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;tRansport tools for the World (Health Organization)&lt;/td&gt;  &lt;td&gt;applications, reproducibility, community/education, space/time, big data&lt;/td&gt;  &lt;td&gt;Paula Andrea&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 84collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The World Health Organization (WHO) contracted us to provide actionable evidence for the redesign of urban transport policies to help rather than hinder human health. That means more active transport.  Designing cost-effective policies to get people walking and cycling requires insight into where, when, how, and why people currently travel.  This is challenging, especially in cities with limited resources, data, or analysis capabilities.  We briefly describe some technical details of our 'Active Transport Toolkit' (ATT), but the primary focus will be the context that led to the WHO contract and where we plan to go next.  We argue that useRs are well-placed to provide openly available, global-scale, transparent tools for policy making.  It was the flexibility of the R language and the supportiveness of its community - notably including ROpenSci, which hosts two of our packages - that enabled us to develop the ATT in a way that makes it flexible enough to capture citys' unique characteristics while providing a consistent user interface.  The talk will conclude with a outline of lessons learned from the perspective of others wanting to create R tools to inform policy.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;62&quot; data-target=&quot;.62collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Applications in health and environment&lt;/td&gt;  &lt;td&gt;Philip Dyer&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;Models of global marine biodiversity: an exercise in mixing R markdown, parallel processing and caching on supercomputers&lt;/td&gt;  &lt;td&gt;models, applications, reproducibility, performance, big data&lt;/td&gt;  &lt;td&gt;Paula Andrea&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://uq-my.sharepoint.com/:p:/g/personal/uqpdyer_uq_edu_au/EaIrZJAOJCBHs3hL1XNvq7QB178XY19_O9cE1JM5hOm7VA&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 62collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;R has become the standard language in ecology for statistics and modelling. If a technique has been published in mathematical ecology it has an R package. Even the data sets have an R package! The size of data sets in ecology has been growing to the point where global analysis of ecological data can be considered. At the same time powerful statistical techniques that rely on randomly permuting the data, such as bootstrapping, have become more popular. These are exciting times, but how do we get R to process our large data sets with computationally expensive algorithms without waiting forever to get results? For those new to R, or at least new to big data in R, I have some tips, techniques and packages to help you get going. I have benefited from using R markdown and Knitr to make short transcript files. I have also made use of caching to avoid recalculating big models and using parallel processing to calculate the models faster in the first place.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;177&quot; data-target=&quot;.177collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Applications in health and environment&lt;/td&gt;  &lt;td&gt;Chris Hansen&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;Enabling Analysts: Embracing R in a National Statistics Office&lt;/td&gt;  &lt;td&gt;Official Statistics&lt;/td&gt;  &lt;td&gt;Paula Andrea&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 177collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Stats NZ has recently adopted R as an approved analytical tool, and more recently for use in production of official outputs.  Since adoption, R has had significant uptake, and has been a great enabler for analysts.  R is more expressive and flexible than the existing tools, allowing them to more easily solve a variety of problems.  R is deployed on powerful servers, so users have a generous supply of memory and cores, meaning large datasets can be handled, and long-running computations parallelised.  Analysts access R using R Studio Server, and this IDE itself has had a number of positive impacts--the use of RStudio projects and R markdown documents in particular helps analysts work in a more organised way, and ensures work is reproducible.  Our statistical platforms can now also use R.  This is done via OpenCPU which enables remote exection of function via an HTTP API.  That is, OpenCPU can be used to call functions in internally developed packages as web services. This has proven useful as we transition to a more service oriented architecture.  In this talk we describe the R environment at Stats NZ, it’s implications for analysts, and provide examples of its use in practice.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;231&quot; data-target=&quot;.231collapsed&quot;&gt;  &lt;td&gt;11:30&lt;/td&gt;  &lt;td&gt;Applications in health and environment&lt;/td&gt;  &lt;td&gt;Tracy Huang&lt;/td&gt;  &lt;td&gt;P8&lt;/td&gt;  &lt;td&gt;Developing an Uncertainty Toolbox for Agriculture: a closer look at Sensitivity Analysis&lt;/td&gt;  &lt;td&gt;visualisation, applications, web app, space/time, big data, R6 and Reference Classes&lt;/td&gt;  &lt;td&gt;Paula Andrea&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 231collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Digiscape is one of 8 Future Science Platforms in CSIRO focussed on delivering new analytics in the digital age to better inform agricultural systems in the face of uncertainty. The Uncertainty Toolbox is one of 15 projects within Digiscape trying to make a difference to the way models are interpreted, reported and communicated in practice for decision-making. Uncertainty is front and centre of every modelling problem but it is sometimes difficult to quantify and challenging to communicate. The Sensitivity Analysis workflow focuses on developing a general framework for sensitivity analysis to inform the modeller about key parameters of interest and refine the model so it can be used in a robust way to make predictions and forecasts with uncertainties. We focus on methods applicable for large scale, non-monotonic problems that develop variance based approaches to sensitivity analysis using emulators. As such, the framework for developing this workflow in R becomes important for transparency and usability.  We will outline the design steps for constructing this workflow using the latest object oriented systems available in R and give a demonstration of the tool using Shiny.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;99&quot; data-target=&quot;.99collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Models and methods for biology and beyond&lt;/td&gt;  &lt;td&gt;Zachary Foster&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Taxa and metacoder:  R packages for parsing, visualization, and manipulation of taxonomic data&lt;/td&gt;  &lt;td&gt;visualisation, data mining, applications, databases, bioinformatics, Taxonomy&lt;/td&gt;  &lt;td&gt;Anna Quaglieri&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 99collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Modern microbiome research is producing datasets that are difficult to manipulate and visualize due to the hierarchical nature of taxonomic classifications. The “taxa” package provides a set of classes for the storage and manipulation of taxonomic data. Classes range from simple building blocks to project-level objects storing multiple user-defined datasets mapped to a taxonomy. It includes parsers that can read in taxonomic information in nearly any form. It also provides functions modeled after dplyr for manipulating a taxonomy and associated datasets such that hierarchical relationships between taxa as well as mappings between taxa and data are preserved. We hope taxa will provide a basis for an ecosystem of compatible packages. We have also developed the metacoder package for visualizing hierarchical data. Metacoder implements a novel visualization called heat trees that use the color and size of nodes and edges on a taxonomic tree to quantitatively depict up to 4 statistics. This allows for rapid exploration of data and information-dense, publication-quality graphics. This is an alternative to the stacked barcharts typically used in microbiome research.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;86&quot; data-target=&quot;.86collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Models and methods for biology and beyond&lt;/td&gt;  &lt;td&gt;Saswati Saha&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Multiple testing approaches for evaluating the effectiveness of a drug combination in a multiple-dose factorial design.&lt;/td&gt;  &lt;td&gt;applications, multivariate, Factorial Design, Drug Combination&lt;/td&gt;  &lt;td&gt;Anna Quaglieri&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 86collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Drug combination trials are often motivated from the fact that using existing drugs in combination might prove to be more productive than the existing drug alone and less expensive than producing an entirely new drug. Several approaches have been explored for developing statistical methods that compare fixed (single) dose combinations to its component.  However, the extension of these approaches to a multiple dose combination clinical trial is not always so simple. Considering these facts we have proposed three approaches by which we can provide confirmatory assurance that combination of two or more drugs is more effective than the component drug alone. These approaches involved multiple comparisons in multilevel factorial design where the type 1 error is controlled by bonferroni test, bootstrap test, and a union intersection test where the least favorable null configuration has been considered. We have also built a R package implementing the above approaches and in this presentation we would like to  demonstrate how this R package can be used in a drug combination trial. We will also demonstrate how these three approaches are performing when benchmarked with an existing approach.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;31&quot; data-target=&quot;.31collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Models and methods for biology and beyond&lt;/td&gt;  &lt;td&gt;Bill Lattner&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Modeling Heterogeneous Treatment Effects with R&lt;/td&gt;  &lt;td&gt;models, applications&lt;/td&gt;  &lt;td&gt;Anna Quaglieri&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://speakerdeck.com/wlattner/modeling-heterogeneous-treatment-effects-with-r&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 31collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Randomized experiments have become ubiquitous in many fields. Traditionally, we have focused on reporting the average treatment effect (ATE) from such experiments. With recent advances in machine learning, and the overall scale at which experiments are now conducted, we can broaden our analysis to include heterogeneous treatment effects. This provides a more nuanced view of the effect of a treatment or change on the outcome of interest. Going one step further, we can use models of heterogeneous treatment effects to optimally allocate treatment.In this talk will provide a brief overview of heterogeneous treatment effect modeling. We will show how to apply some recently proposed methods using R, and compare the results of each using a question wording experiment from the General Social Survey. Finally, we will conclude with some practical issues in modeling heterogeneous treatment effects, including model selection and obtaining valid confidence intervals.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;187&quot; data-target=&quot;.187collapsed&quot;&gt;  &lt;td&gt;11:30&lt;/td&gt;  &lt;td&gt;Models and methods for biology and beyond&lt;/td&gt;  &lt;td&gt;Shian Su&lt;/td&gt;  &lt;td&gt;P9&lt;/td&gt;  &lt;td&gt;Glimma: interactive graphics for gene expression analysis&lt;/td&gt;  &lt;td&gt;visualisation, applications, bioinformatics&lt;/td&gt;  &lt;td&gt;Anna Quaglieri&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 187collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Modern RNA sequencing produces large amounts of data containing tens of thousands of genes. Exploratory and statistical analysis of these genes produces plots or tables with many data points. Glimma is a Bioconductor package that provides interactive versions of common plots from limma, a widely used gene expression analysis package. It allows researchers to explore the statistical summary of their data, with cross-chart interactions providing greater insight into the behaviours of specific genes. Interactivity allows genes of interest to be quickly interrogated on the summary graphic which provides better context than searching through spreadsheets. Cross-chart interactions display useful additional content that would otherwise require manual querying. Glimma produces HTML pages with custom D3 Javascript that handles interactions completely independent from R, the resulting plots to easily be shared with researchers without the need for software dependencies beyond a modern browser.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;85&quot; data-target=&quot;.85collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Learning and teaching&lt;/td&gt;  &lt;td&gt;François Michonneau&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;Lessons learned from developing R-based curricula across disciplines&lt;/td&gt;  &lt;td&gt;community/education&lt;/td&gt;  &lt;td&gt;Sam Clifford&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/e/2PACX-1vTW-p1EA6ypLg39m0YcNhy3keb7x4j30s4stYabohjjIWO-Dl7-yJivyEvvCowcYTQ3qRtWaDrQtm-C/pub?start=false&amp;loop=false#slide=id.p&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 85collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The Carpentries is a non-profit volunteer organization that teaches scientists with no or little programming experience foundational skills in coding, data science, and best-practices for reproducible research. We offer 2-day workshops for a variety of disciplines including Ecology, Genomics, Geospatial analysis, and Social Sciences. With 1300+ instructors who have taught 500+ workshops on all continents, we worked with our community of instructors to assemble evidence-based curricula using results from research on teaching and learning. We have developed detailed short- and long-term assessments to evaluate the effectiveness and level of satisfaction of our learners after attending a workshop, as well as the impact on their research and careers 6 months or more afterwards. We find that workshop participants program more often, are more confident, and use programming practices that the report make them more efficient and reproducible. Here, we will present the lessons we learned about developing curricula based on teaching R to novices across diverse disciplines, and the strategies we use to instill the desire to continue learning after attending our workshops.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;19&quot; data-target=&quot;.19collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Learning and teaching&lt;/td&gt;  &lt;td&gt;Matthias Gehrke&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;Student Performance and Acceptance of Technology in a Statistics Course Based on R mosaic - Results from a Pre- and Post-Test Survey&lt;/td&gt;  &lt;td&gt;community/education, teaching&lt;/td&gt;  &lt;td&gt;Sam Clifford&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://www.dropbox.com/s/zeum65rzw8lrhwp/useR2018_Gehrke_Luebke.pdf?dl=0&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 19collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;In the last years there is movement towards simulation-based inference (e.g., bootstrapping and randomization tests) in order to improve students' understanding of statistical reasoning (see e.g. Chance et al. 2016). The R package mosaic was developed with a &quot;minimal R&quot; approach to simplify the introduction of these concepts (Pruim et al. (2017)). With a pre and post survey we analysed whether students improved in understanding as well as in acceptance of R during a one semester statistics course in economically related Bachelor and Master programs. These courses were held by different lecturers at multiple locations in Germany. At our private university of applied sciences for professionals studying while working the use of R is compulsory in all statistical courses.While conceptual understanding was evaluated by a subset of the modified CAOS inventory (like Chance et al. (2016)) the acceptance and use of technology was collected by using an adopted version of UTAUT2 (Venkatesh et al. (2012)).&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;25&quot; data-target=&quot;.25collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Learning and teaching&lt;/td&gt;  &lt;td&gt;Mette Langaas&lt;/td&gt;  &lt;td&gt;P7&lt;/td&gt;  &lt;td&gt;Teaching statistics - with all learning resources written in R Markdown&lt;/td&gt;  &lt;td&gt;community/education, teaching&lt;/td&gt;  &lt;td&gt;Sam Clifford&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://folk.ntnu.no/mettela/Talks/useR2018ML.html&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 25collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;In applied courses in statistics it is important for the student to see a mix of theory, practical examples and data analyses. Being able to study the R code used to produce the data analyses, and to run and modify the R code will give the student hands on experience, which again may lead to increased theoretical understanding.I will tell about my experiences with producing and using learning material written in R Markdown in two courses in statistics at the Norwegian University of Science and Technology. One course is at the master level (Generalized linear models) with few students (35) and a mix of plenary and interactive lectures. The other course is at the bachelor level (Statistical learning) with more students (70). &lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;66&quot; data-target=&quot;.66collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Data handling&lt;/td&gt;  &lt;td&gt;Chester Ismay&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Statistical Inference: A Tidy Approach using R&lt;/td&gt;  &lt;td&gt;visualisation, community/education, statistical inference, tidyverse community&lt;/td&gt;  &lt;td&gt;Jenny Bryan&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;http://bit.ly/ismay-useR&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 66collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;How do you code-up a permutation test in R? What about an ANOVA or a chi-square test? Have you ever been uncertain as to exactly which type of test you should run given the data and questions asked? The `infer` R package was created to unite common statistical inference tasks into an expressive and intuitive framework to alleviate some of these struggles and make inference more intuitive. This talk will focus on developing an understanding of the design principles of the package, which are firmly motivated by Hadley Wickham's tidy tools manifesto. It will also discuss the implementation, centered on the common conceptual threads that link a surprising range of hypothesis tests and confidence intervals. Lastly, we'll dive into some examples of how to implement the code of the `infer` package via different data sets and variable scenarios. The package is aimed to be useful to new students of statistics as well as seasoned practitioners.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;81&quot; data-target=&quot;.81collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Data handling&lt;/td&gt;  &lt;td&gt;Thomas Lumley&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Subsampling and one-step polishing for generalised linear models&lt;/td&gt;  &lt;td&gt;algorithms, models, databases, big data&lt;/td&gt;  &lt;td&gt;Jenny Bryan&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 81collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Using only a commodity laptop it's possible to fit a generalised linear model to a dataset from about a million to a billion rows by first fitting to a subset and then doing a one-step update. The method depends on a bit of asymptotic theory, some sampling, the Fisher scoring algorithm, efficient R-database interfaces, and a little of the tidyverse.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;159&quot; data-target=&quot;.159collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Data handling&lt;/td&gt;  &lt;td&gt;James Hester&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Glue strings to data in R&lt;/td&gt;  &lt;td&gt;Package development&lt;/td&gt;  &lt;td&gt;Jenny Bryan&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 159collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;String interpolation, evaluating a variable name to a value within a string, isa feature of many programming languages including Python, Julia, Javascript,Rust, and most Unix Shells. R's `sprintf()` and `paste()` functions providesome of this functionality, but have limitations which make them cumbersome touse. There are also some existing add on packages with similar functionality,however each has drawbacks.The glue package &lt;http://glue.tidyverse.org/&gt; performs robust stringinterpolation for R. This includes evaluation of variables and arbitrary R code,with a clean and simple syntax. Because it is dependency-free, it is easy toincorporate into packages. In addition, glue provides an extensible interfaceto perform more complex transformations; such as `glue_sql()` to constructSQL queries with automatically quoted variables.This talk will show how to utilize glue to write beautiful code which iseasy to read, write and maintain. We will also discuss ways to best use glue whenperformance is a concern. Finally we will create custom glue functions tailoredtowards specific use cases, such as JSON construction, colored messages, emojiinterpolation and more.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;132&quot; data-target=&quot;.132collapsed&quot;&gt;  &lt;td&gt;11:30&lt;/td&gt;  &lt;td&gt;Data handling&lt;/td&gt;  &lt;td&gt;Max Kuhn&lt;/td&gt;  &lt;td&gt;AUD&lt;/td&gt;  &lt;td&gt;Data Preprocessing using Recipes&lt;/td&gt;  &lt;td&gt;algorithms, models&lt;/td&gt;  &lt;td&gt;Jenny Bryan&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://github.com/topepo/user2018&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 132collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The recipes package can be used as a replacement for model.matrix as well as a general feature engineering tool. The package uses a dplyr-like syntax where a specification for a sequence of data preprocessing steps are created with the execution of these steps deferred until later. Data processing recipes can be created sequentially and intermediate results can be cached. An example is used to illustrate the basic recipe functionality and philosophy.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;100&quot; data-target=&quot;.100collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Statistical modeling&lt;/td&gt;  &lt;td&gt;John Fox&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;New Features in the car and effects Packages&lt;/td&gt;  &lt;td&gt;visualisation, models&lt;/td&gt;  &lt;td&gt;Matteo Fasiolo&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://socialsciences.mcmaster.ca/jfox/Papers/Fox-Price-Weisberg-useR2018-notes.pdf&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 100collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The widely used car and effects packages are associated with Foxand Weisberg, An R Companion to Applied Regression, the thirdedition of which will be published this year. In preparation, wehave released the substantially revised version 3.0-0 of the carpackage and version 4.0-1 of the effects package.The car package focuses on tools, many of them graphical, that areuseful for applied regression analysis (linear, generalized linear, mixed-effects models, etc.), including tools for preparing, examining, and transformingdata prior to specification of a regression model, and tools thatare useful for assessing regression models that have been fit todata. The effects packages focuses on graphical methods forinterpreting regression models that have been fit to data.Among the many changes and improvements to the packages are areconceptualization of effect displays, which we call &quot;predictoreffects&quot;; the ability to add partial residuals to effect plots ofarbitrary complexity; simplification to the arguments of plottingfunctions; new and improved functions for summarizing and testingstatistical models; and improved methods for selecting variabletransformations.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;106&quot; data-target=&quot;.106collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Statistical modeling&lt;/td&gt;  &lt;td&gt;Rainer Hirk&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;mvord: An R Package for Fitting Multivariate Ordinal Regression Models&lt;/td&gt;  &lt;td&gt;algorithms, models, applications, multivariate&lt;/td&gt;  &lt;td&gt;Matteo Fasiolo&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 106collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;The R package mvord implements composite likelihood estimation in the class of multivariate ordinal regression models with probit and a logit link. A flexible modeling framework for multiple ordinal measurements on the same subject is set up, which takes into consideration the dependence among the multiple observations by employing different error structures. Heterogeneity in the error structure across the subjects can be accounted for by the package, which allows for covariate dependent error structures. In addition, regression coefficients and threshold parameters are varying across the multiple response dimensions in the default implementation. However, constraints can be defined by the user if areduction of the parameter space is desired. The proposed multivariate framework is illustrated by means of a credit risk application.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;103&quot; data-target=&quot;.103collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Statistical modeling&lt;/td&gt;  &lt;td&gt;Joachim Schwarz&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;Partial Least Squares with formative constructs and a binary target variable&lt;/td&gt;  &lt;td&gt;PLS, pslpm package, formative constructs, binary target variable&lt;/td&gt;  &lt;td&gt;Matteo Fasiolo&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 103collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;During the last years, the use of PLS became more and more important for the modelling of dependencies between latent variables as an alternative to classical structural equation modelling. However, a non-metric target variable in combination with formatively measured constructs is still a particular challenge for the PLS-approach.By using the plspm package (Sanchez/Trinchera/Russolillo 2017), we tested a model from the human resources management field. Main goal of this model is to examine the moderating and mediating role of meaning at work for the relationship between several social, personal, environmental and motivational job characteristics and the intention to quit as a manifest binary target variable. Coping with the complexity of the model, consisting of more than 70 latent variables, all formatively measured, many of them one indicator constructs, there are some pitfalls in the application of the plspm package, but due to the flexibility of R, it is possible even to evaluate such a complex model.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;192&quot; data-target=&quot;.192collapsed&quot;&gt;  &lt;td&gt;11:30&lt;/td&gt;  &lt;td&gt;Statistical modeling&lt;/td&gt;  &lt;td&gt;Murray Cameron&lt;/td&gt;  &lt;td&gt;P10&lt;/td&gt;  &lt;td&gt;Exceeding the designer's expectation&lt;/td&gt;  &lt;td&gt;algorithms, models, applications&lt;/td&gt;  &lt;td&gt;Matteo Fasiolo&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;https://drive.google.com/drive/folders/1eCiVtVYzhfhwsTy1Itq3hcqI3qv6HPzB?usp=sharing&quot;&gt;click here&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 192collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Statistical methods and their software implementation are generally designed for a particular class of applications. However, the nature of data, analysis and statisticians is that uses of the methods are envisaged that extend the application.  Sometimes the reason is the nature of the data, sometimes it is a new type of model and sometimes it is the limitations of the software available. Software for regression and for generalised linear models have regularly been used in 'non-standard' ways.We will discuss some examples, considering some changepoint models in particular and emphasise some old lessons for software developers.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;50&quot; data-target=&quot;.50collapsed&quot;&gt;  &lt;td&gt;10:30&lt;/td&gt;  &lt;td&gt;Better data performance&lt;/td&gt;  &lt;td&gt;David Cooley&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;Starting with geospatial data in Shiny, and knowing when to stop&lt;/td&gt;  &lt;td&gt;visualisation, databases, web app, performance, spatial&lt;/td&gt;  &lt;td&gt;David Smith&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 50collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Theme:Coupling R with geospatial databases to reduce the calculations and data in R and improve shiny app speedLike any web page, Shiny apps need to be quick and responsive for a better user experience. Doing complex calculations and storing large data objects will slow the app.  Therefore, it's often desirable to remove as much of this as possible from the app.  The talk will demonstrate- Using MongoDB as a geospatial database- Querying &amp; returning geospatial data to R from MongoDB- Comparison and benchmarking of geospatial operations in R vs on the database server- Applying this to a shiny app with a demonstration, highlighting the pros &amp; cons- Introducing the latest updates to the `googleway` package for displaying data and using Google Map tools through R- Using Google Maps to trigger database queries and operations&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;68&quot; data-target=&quot;.68collapsed&quot;&gt;  &lt;td&gt;10:50&lt;/td&gt;  &lt;td&gt;Better data performance&lt;/td&gt;  &lt;td&gt;Jeffrey O. Hanson&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;prioritizr: Systematic conservation prioritization in R&lt;/td&gt;  &lt;td&gt;reproducibility, space/time, performance, conservation&lt;/td&gt;  &lt;td&gt;David Smith&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 68collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Biodiversity is in crisis. To prevent further declines, protected areas need to be established in places that will achieve conservation objectives for minimal cost. However, existing decision support tools tend to offer limited customizability and can take a long time to deliver solutions. To overcome these limitations and help prioritize conservation efforts in a transparent and reproducible manner, here we present the prioritizr R package. Inspired by the tidyverse principles, this R package provides a flexible interface for articulating, building and solving conservation planning problems. In contrast to existing tools, the prioritizr R package uses integer linear programming (ILP) techniques to mathematically formulate and solve conservation problems. As a consequence, the prioritizr R package can find solutions that are guaranteed to be optimal and in record time. By finding solutions to problems that are relevant to the species, ecosystems, and economic factors in areas of interest, conservation scientists, planners, and decision makers stand a far greater chance at enhancing biodiversity. For more information, visit https://github.com/prioritizr/prioritizr.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;173&quot; data-target=&quot;.173collapsed&quot;&gt;  &lt;td&gt;11:10&lt;/td&gt;  &lt;td&gt;Better data performance&lt;/td&gt;  &lt;td&gt;Remy Gavard&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;Using R to pre-process ultra-high-resolution mass spectrometry data of complex mixtures.&lt;/td&gt;  &lt;td&gt;algorithms, applications&lt;/td&gt;  &lt;td&gt;David Smith&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 173collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Scientists are able to determine over hundreds of thousands of components in crude oil using Fourier transform ion cyclotron resonance mass spectrometry (FTICR-MS). The statistical tools required to analyse the mass spectra struggle to keep pace with advancinginstrument capabilities and increasing quantities of data. Today most ultrahigh resolution analyses for complex mixture samples are based on single, labour-intensive, experiments.We present a new algorithm developed in R named Themis to jointly pre-process replicate measurements of a complex sample analysed using FTICR-MS. This improves consistency as a preliminary step to assigning chemical compositions, and the algorithm has a quality control criterion. Through the use of peak alignment and an adaptive mixture model-based strategy, it is possible to distinguish true peaks from noise.Themis demonstrated a more effective removal of noise-related peaks and the preservation and improvement of the chemical composition profile. Themis enabled the isolation of peaks that would have otherwise been discarded using traditional peak picking (based upon signal-to-noise ratio alone) for a single spectrum.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;136&quot; data-target=&quot;.136collapsed&quot;&gt;  &lt;td&gt;11:30&lt;/td&gt;  &lt;td&gt;Better data performance&lt;/td&gt;  &lt;td&gt;Joshua Bon&lt;/td&gt;  &lt;td&gt;P6&lt;/td&gt;  &lt;td&gt;Semi-infinite programming in R&lt;/td&gt;  &lt;td&gt;algorithms, models&lt;/td&gt;  &lt;td&gt;David Smith&lt;/td&gt;  &lt;td&gt;&lt;a href=&quot;NA&quot;&gt;NA&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=&quot;collapse out budgets 136collapsed&quot;&gt;  &lt;td colspan=&quot;8&quot;&gt;Semi-infinite programming (SIP) is an optimisation problem where, generally, there are a finite number of variables but an infinite number of (parametrised) constraints. We show how to optimise simple SIP problems in R, in particular  SIP for shape-constrained regression. The package sipr (under development) will be presented and collaboration sought from those in attendance.&lt;/td&gt;&lt;/tr&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<pre><code>  &lt;tr class=&quot;clickable&quot; data-toggle=&quot;collapse&quot; id=&quot;4&quot; data-target=&quot;.4collapsed&quot;&gt;
</code></pre>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

                        </div>

                    </div>
                    <div class="col-md-3 hidden-xs hidden-sm">
                      <ul class="nav nav-pills nav-stacked">
                        <nav id="TableOfContents"></nav>
                      </ul>
                    </div>

                </div>
                

            </div>
            

            
        </div>
        

        <footer id="footer">
    <div class="container">

        
        <div class="col-md-4 col-sm-6">
            <h4>Local organising committee</h4>

            <!-- raw HTML omitted -->

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

        <div class="col-md-4 col-sm-6">

             
            <h4>Recent news</h4>

            <div class="blog-entries">
                
                
                
                
                
                
                
                
                
                    
                <h5><a href="https://user2018.r-project.org/blog/2018/07/10/wifi/">Conference wifi</a></h5>
                    
                
                
                
                
                
                
                
                
                
                
                    
                <h5><a href="https://user2018.r-project.org/blog/">News</a></h5>
                    
                
                
                
                
                
                
                
                
                
                
                    
                <h5><a href="https://user2018.r-project.org/blog/2018/06/29/lastminute/">Last minute details</a></h5>
                    
                
                
                
                
                
                
                
                
                
                
                    
                <h5><a href="https://user2018.r-project.org/blog/2018/05/30/interview-with-steph-de-silva/">Interview with Steph de Silva</a></h5>
                    
                
                
                
                
                
                
                
                
                
                
                    
                <h5><a href="https://user2018.r-project.org/blog/2018/05/17/updates/">Updates and news</a></h5>
                    
                
                
            </div>

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        
        <div class="col-md-4 col-sm-6">

          <h4>Venue</h4>
          

            <!-- raw HTML omitted -->

            <a href="/contact" class="btn btn-small btn-template-main">Go to contact page</a>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-4 col-sm-6">
        
          
          <p class="pull-left"><p>Copyright &copy; 2017-2018, <strong><a href='https://www.r-project.org/foundation'>R Foundation</a></strong></p></p>
            
        </div>
        <div class="col-md-4 col-sm-6">
          <p class="pull-left">
          <a href="https://twitter.com/useR2018_conf">
            <i class="fa fa-twitter fa-2x" aria-hidden="true"></i>
          </a>
          <a href="https://twitter.com/useR2018_conf">@useR2018_conf</a>
           //
          <a href="https://twitter.com/hashtag/useR2018?src=hash">#useR2018</a>
          </p>
        </div>
        <div class="col-md-4 col-sm-6">
            Template by <a href="http://bootstrapious.com/free-templates">Bootstrapious</a>.
            

            Ported to Hugo by <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>
        </div>
    </div>
</div>





    </div>
    

    
<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?v=3.exp"></script>

<script src="https://user2018.r-project.org/js/hpneo.gmaps.js"></script>
<script src="https://user2018.r-project.org/js/gmaps.init.js"></script>
<script src="https://user2018.r-project.org/js/front.js"></script>


<script src="https://user2018.r-project.org/js/owl.carousel.min.js"></script>


  </body>
</html>
